Base Model: DistilBERT (Transformer-based LLM)

Dataset: WELFake (72,134 labeled articles)

Training Split: 80% Train / 20% Test

Final Test Accuracy: [Insert your % here, eg, 92.4%]

Hybrid Logic: Includes a Google Fact Check API override to correct "Sophisticated Misinformation."
